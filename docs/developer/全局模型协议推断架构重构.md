# 全局模型协议推断架构重构

## 目标

抽象一个通用的模型协议推断系统，让每个 Channel 类型可以实现自己的推断逻辑，同时提供全局的模型名称推断作为后备方案。

## 核心概念

### 1. 模型协议类型定义

定义一个统一的模型协议类型，用于描述模型的 API 协议：

```typescript
/**
 * 模型协议类型
 * 描述模型使用的 API 协议标准
 */
export type ModelProtocol =
  | 'anthropic' // Anthropic Messages API
  | 'openai' // OpenAI Chat Completions API
  | 'google-ai' // Google AI (Gemini) API
  | 'openai-compatible' // OpenAI 兼容 API (通用)

/**
 * 模型协议信息
 */
export interface ModelProtocolInfo {
  protocol: ModelProtocol
  baseUrl: string
  requiresTransform?: boolean // 是否需要 URL 转换
}
```

### 2. Channel 推断接口

定义 Channel 级别的推断能力接口：

```typescript
/**
 * Channel 推断上下文
 */
export interface ChannelInferenceContext {
  channelType: ChannelType
  platform: string | null
  baseUrl: string
}

/**
 * Channel 推断器接口
 * 每个 Channel 类型可以实现自己的推断逻辑
 */
export interface ChannelInferrer {
  /**
   * 基于 Channel 自身信息推断协议
   * 返回 null 表示无法从 Channel 推断，需要使用模型名称推断
   */
  inferFromChannel(context: ChannelInferenceContext): ModelProtocol | null

  /**
   * 基于模型 ID 推断协议
   * 返回 null 表示无法推断，使用全局推断
   */
  inferFromModel(
    modelId: string,
    context: ChannelInferenceContext
  ): ModelProtocol | null

  /**
   * 获取协议对应的 Base URL
   */
  getBaseUrl(
    protocol: ModelProtocol,
    baseUrl: string,
    platform?: string | null
  ): string
}
```

### 3. 全局模型名称推断

作为最终的后备方案：

```typescript
/**
 * 全局模型名称推断规则
 * 基于模型 ID 的前缀/模式匹配
 */
export function inferProtocolFromModelId(modelId: string): ModelProtocol {
  const lowerModelId = modelId.toLowerCase()

  // Claude 系列
  if (lowerModelId.startsWith('claude-')) return 'anthropic'

  // GPT 系列
  if (lowerModelId.startsWith('gpt-')) return 'openai'
  if (lowerModelId.startsWith('o1-')) return 'openai'
  if (lowerModelId.startsWith('o3-')) return 'openai'

  // Gemini 系列
  if (lowerModelId.startsWith('gemini-')) return 'google-ai'

  // 默认使用 OpenAI 兼容协议
  return 'openai-compatible'
}
```

## 实现架构

### 文件结构

```
src/lib/
├── model-protocol/
│   ├── index.ts                    # 导出所有公共接口
│   ├── types.ts                    # 类型定义
│   ├── global-inference.ts         # 全局模型名称推断
│   ├── inferrer-registry.ts        # 推断器注册表
│   └── channel-inferrers/
│       ├── sub2api-inferrer.ts     # Sub2API 推断器
│       ├── newapi-inferrer.ts      # NewAPI 推断器
│       └── cliproxy-inferrer.ts    # CLI Proxy API 推断器
```

### 核心实现

#### 1. `types.ts` - 类型定义

```typescript
import type { ChannelType } from '@/lib/bindings'

export type ModelProtocol =
  | 'anthropic'
  | 'openai'
  | 'google-ai'
  | 'openai-compatible'

export interface ModelProtocolInfo {
  protocol: ModelProtocol
  baseUrl: string
}

export interface ChannelInferenceContext {
  channelType: ChannelType
  platform: string | null
  baseUrl: string
}

export interface ChannelInferrer {
  inferFromChannel(context: ChannelInferenceContext): ModelProtocol | null
  inferFromModel(
    modelId: string,
    context: ChannelInferenceContext
  ): ModelProtocol | null
  getBaseUrl(
    protocol: ModelProtocol,
    baseUrl: string,
    platform?: string | null
  ): string
}
```

#### 2. `global-inference.ts` - 全局推断

```typescript
import type { ModelProtocol } from './types'

/**
 * 全局模型名称推断
 * 基于模型 ID 的前缀/模式匹配
 */
export function inferProtocolFromModelId(modelId: string): ModelProtocol {
  const lowerModelId = modelId.toLowerCase()

  // Claude 系列 (Anthropic)
  if (lowerModelId.startsWith('claude-')) return 'anthropic'

  // GPT 系列 (OpenAI)
  if (lowerModelId.startsWith('gpt-')) return 'openai'
  if (lowerModelId.startsWith('o1-')) return 'openai'
  if (lowerModelId.startsWith('o3-')) return 'openai'

  // Gemini 系列 (Google AI)
  if (lowerModelId.startsWith('gemini-')) return 'google-ai'

  // 默认使用 OpenAI 兼容协议
  return 'openai-compatible'
}
```

#### 3. `channel-inferrers/sub2api-inferrer.ts` - Sub2API 推断器

```typescript
import type {
  ChannelInferrer,
  ChannelInferenceContext,
  ModelProtocol,
} from '../types'
import { inferProtocolFromModelId } from '../global-inference'

export class Sub2ApiInferrer implements ChannelInferrer {
  inferFromChannel(context: ChannelInferenceContext): ModelProtocol | null {
    // Sub2API 的 platform 字段直接表明协议类型
    const { platform } = context

    if (platform === 'openai') return 'openai'
    if (platform === 'anthropic') return 'anthropic'
    if (platform === 'gemini') return 'google-ai'

    // antigravity 需要根据模型名称判断，返回 null 让模型推断处理
    if (platform === 'antigravity') return null

    // 未知 platform，返回 null 使用模型推断
    return null
  }

  inferFromModel(
    modelId: string,
    context: ChannelInferenceContext
  ): ModelProtocol | null {
    const { platform } = context

    // antigravity 特殊处理：支持 Claude 和 Gemini
    if (platform === 'antigravity') {
      if (modelId.startsWith('claude-')) return 'anthropic'
      if (modelId.startsWith('gemini-')) return 'google-ai'
      return 'anthropic' // 默认 Claude
    }

    // 其他情况返回 null，使用全局推断
    return null
  }

  getBaseUrl(
    protocol: ModelProtocol,
    baseUrl: string,
    platform?: string | null
  ): string {
    // antigravity 特殊路径处理
    if (platform === 'antigravity') {
      if (protocol === 'anthropic') {
        return this.normalizeBaseUrl(baseUrl, '/antigravity')
      }
      if (protocol === 'google-ai') {
        return this.normalizeBaseUrl(baseUrl, '/antigravity/v1beta')
      }
    }

    // Sub2API 不需要额外的路径转换
    return baseUrl
  }

  private normalizeBaseUrl(baseUrl: string, suffix: string): string {
    const trimmed = baseUrl.replace(/\/+$/, '')
    return trimmed.endsWith(suffix) ? trimmed : `${trimmed}${suffix}`
  }
}
```

#### 4. `channel-inferrers/newapi-inferrer.ts` - NewAPI 推断器

```typescript
import type {
  ChannelInferrer,
  ChannelInferenceContext,
  ModelProtocol,
} from '../types'

export class NewApiInferrer implements ChannelInferrer {
  inferFromChannel(_context: ChannelInferenceContext): ModelProtocol | null {
    // NewAPI 无法从 Channel 推断，必须使用模型名称
    return null
  }

  inferFromModel(
    modelId: string,
    _context: ChannelInferenceContext
  ): ModelProtocol | null {
    // NewAPI 使用简单的模型名称推断
    if (modelId.startsWith('claude-')) return 'anthropic'
    if (modelId.startsWith('gpt-')) return 'openai'

    // 其他返回 null，使用全局推断
    return null
  }

  getBaseUrl(protocol: ModelProtocol, baseUrl: string): string {
    // Anthropic 不需要 /v1 后缀
    if (protocol === 'anthropic') return baseUrl

    // 其他协议添加 /v1
    return this.normalizeBaseUrl(baseUrl, '/v1')
  }

  private normalizeBaseUrl(baseUrl: string, suffix: string): string {
    const trimmed = baseUrl.replace(/\/+$/, '')
    return trimmed.endsWith(suffix) ? trimmed : `${trimmed}${suffix}`
  }
}
```

#### 5. `channel-inferrers/cliproxy-inferrer.ts` - CLI Proxy API 推断器

```typescript
import { NewApiInferrer } from './newapi-inferrer'

/**
 * CLI Proxy API 使用与 NewAPI 相同的推断逻辑
 */
export class CliProxyInferrer extends NewApiInferrer {}
```

#### 6. `inferrer-registry.ts` - 推断器注册表

```typescript
import type { ChannelType } from '@/lib/bindings'
import type { ChannelInferrer } from './types'
import { Sub2ApiInferrer } from './channel-inferrers/sub2api-inferrer'
import { NewApiInferrer } from './channel-inferrers/newapi-inferrer'
import { CliProxyInferrer } from './channel-inferrers/cliproxy-inferrer'

const inferrerMap = new Map<ChannelType, ChannelInferrer>([
  ['sub-2-api', new Sub2ApiInferrer()],
  ['new-api', new NewApiInferrer()],
  ['cli-proxy-api', new CliProxyInferrer()],
])

/**
 * 获取指定 Channel 类型的推断器
 */
export function getInferrer(channelType: ChannelType): ChannelInferrer {
  const inferrer = inferrerMap.get(channelType)
  if (!inferrer) {
    throw new Error(`No inferrer registered for channel type: ${channelType}`)
  }
  return inferrer
}
```

#### 7. `index.ts` - 统一推断入口

```typescript
import type { ChannelType } from '@/lib/bindings'
import type {
  ModelProtocol,
  ModelProtocolInfo,
  ChannelInferenceContext,
} from './types'
import { getInferrer } from './inferrer-registry'
import { inferProtocolFromModelId } from './global-inference'

/**
 * 推断模型协议
 *
 * 推断优先级：
 * 1. Channel 自身推断 (inferFromChannel)
 * 2. Channel 的模型推断 (inferFromModel)
 * 3. 全局模型名称推断 (inferProtocolFromModelId)
 *
 * @param channelType - Channel 类型
 * @param platform - Platform 标识
 * @param baseUrl - Base URL
 * @param modelId - 模型 ID (可选)
 * @returns 模型协议类型
 */
export function inferModelProtocol(
  channelType: ChannelType,
  platform: string | null,
  baseUrl: string,
  modelId?: string
): ModelProtocol {
  const context: ChannelInferenceContext = { channelType, platform, baseUrl }
  const inferrer = getInferrer(channelType)

  // 1. 尝试从 Channel 推断
  const channelResult = inferrer.inferFromChannel(context)
  if (channelResult) return channelResult

  // 2. 如果有模型 ID，尝试 Channel 的模型推断
  if (modelId) {
    const modelResult = inferrer.inferFromModel(modelId, context)
    if (modelResult) return modelResult
  }

  // 3. 使用全局模型名称推断
  if (modelId) {
    return inferProtocolFromModelId(modelId)
  }

  // 4. 默认返回 OpenAI 兼容
  return 'openai-compatible'
}

/**
 * 推断模型协议信息（包含 Base URL）
 */
export function inferModelProtocolInfo(
  channelType: ChannelType,
  platform: string | null,
  baseUrl: string,
  modelId?: string
): ModelProtocolInfo {
  const protocol = inferModelProtocol(channelType, platform, baseUrl, modelId)
  const inferrer = getInferrer(channelType)
  const transformedBaseUrl = inferrer.getBaseUrl(protocol, baseUrl, platform)

  return {
    protocol,
    baseUrl: transformedBaseUrl,
  }
}

// 导出类型和工具函数
export * from './types'
export { inferProtocolFromModelId } from './global-inference'
export { getInferrer } from './inferrer-registry'
```

## 使用示例

### 1. 在 ChannelModelPicker 中使用

```typescript
import {
  inferModelProtocol,
  inferModelProtocolInfo,
} from '@/lib/model-protocol'

const inferProvider = (modelId: string): Provider => {
  if (!selectedChannel || !selectedKey) return 'generic-chat-completion-api'

  const protocol = inferModelProtocol(
    selectedChannel.type,
    selectedKey.platform,
    selectedChannel.baseUrl,
    modelId
  )

  // 将 ModelProtocol 映射到 Provider (向后兼容)
  return protocolToProvider(protocol)
}

const getBaseUrl = (provider: Provider): string => {
  if (!selectedChannel || !selectedKey) return ''

  const protocol = providerToProtocol(provider)
  const inferrer = getInferrer(selectedChannel.type)

  return inferrer.getBaseUrl(
    protocol,
    selectedChannel.baseUrl,
    selectedKey.platform
  )
}
```

### 2. 在 OpenCode Provider 导入中使用

```typescript
import { inferModelProtocol } from '@/lib/model-protocol'

const handleImportFromChannel = (
  models: CustomModel[],
  context: ChannelProviderContext
) => {
  if (!isEditing) {
    // 使用第一个模型推断协议
    const firstModelId = models[0]?.model
    const protocol = inferModelProtocol(
      context.channelType,
      context.platform,
      context.baseUrl,
      firstModelId
    )

    // 映射到 OpenCode NPM 包
    const npm = protocolToOpenCodeNpm(protocol)
    setNpm(npm)
  }
}

function protocolToOpenCodeNpm(protocol: ModelProtocol): string {
  const map: Record<ModelProtocol, string> = {
    anthropic: '@ai-sdk/anthropic',
    openai: '@ai-sdk/openai',
    'google-ai': '@ai-sdk/google',
    'openai-compatible': '@ai-sdk/openai-compatible',
  }
  return map[protocol]
}
```

## 迁移策略

### 阶段 1：创建新架构（不破坏现有代码）

1. 创建 `src/lib/model-protocol/` 目录及所有文件
2. 实现所有推断器
3. 添加单元测试

### 阶段 2：逐步迁移现有代码

1. 更新 `ChannelModelPicker` 使用新 API
2. 更新 `ChannelDetail` 使用新 API
3. 更新 `OpenCode ProviderDialog` 使用新 API
4. 更新 `OpenClaw ProviderDialog` 使用新 API

### 阶段 3：废弃旧代码

1. 标记 `sub2api-platform.ts` 和 `newapi-platform.ts` 为 deprecated
2. 在下一个大版本中移除旧文件

## 测试计划

### 单元测试

```typescript
// global-inference.test.ts
describe('inferProtocolFromModelId', () => {
  it('识别 Claude 模型', () => {
    expect(inferProtocolFromModelId('claude-3-opus')).toBe('anthropic')
  })

  it('识别 GPT 模型', () => {
    expect(inferProtocolFromModelId('gpt-4')).toBe('openai')
  })

  it('识别 Gemini 模型', () => {
    expect(inferProtocolFromModelId('gemini-pro')).toBe('google-ai')
  })

  it('未知模型返回 openai-compatible', () => {
    expect(inferProtocolFromModelId('unknown-model')).toBe('openai-compatible')
  })
})

// sub2api-inferrer.test.ts
describe('Sub2ApiInferrer', () => {
  it('从 platform 推断协议', () => {
    const inferrer = new Sub2ApiInferrer()
    expect(
      inferrer.inferFromChannel({
        channelType: 'sub-2-api',
        platform: 'anthropic',
        baseUrl: 'https://api.example.com',
      })
    ).toBe('anthropic')
  })

  it('antigravity 根据模型推断', () => {
    const inferrer = new Sub2ApiInferrer()
    expect(
      inferrer.inferFromModel('claude-3-opus', {
        channelType: 'sub-2-api',
        platform: 'antigravity',
        baseUrl: 'https://api.example.com',
      })
    ).toBe('anthropic')
  })
})
```

## 优势

1. **清晰的职责分离**：每个 Channel 类型有自己的推断器
2. **易于扩展**：新增 Channel 类型只需实现 `ChannelInferrer` 接口
3. **统一的推断逻辑**：所有地方使用相同的推断入口
4. **类型安全**：TypeScript 接口确保实现正确
5. **向后兼容**：可以逐步迁移，不破坏现有代码
6. **可测试性**：每个推断器可以独立测试
